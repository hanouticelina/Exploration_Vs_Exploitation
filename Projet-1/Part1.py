import numpy as np

epsilon = 0.1

 
def operate(machine, lever):
    return bernoulli(machine[lever])

def random_algo(rewards, choices):
    return np.random.randrange(0,rewards.size)

def greedy_algo(rewards, choices):
    return np.argmax(rewards)

def eps_greedy_algo(rewards, choices):
    if bernoulli(epsilon) is True:
        return random_algo(rewards, choices)
    return greedy_algo(rewards, choices)

def ucb_algo(rewards, choices):
    if (np.min(choices) == 0):
        return np.argmin(choices)
    t = np.sum(choices)
    choice_values = rewards + np.sqrt((2 * np.log(t)) / choices)
    return np.argmax(choice_values)
        
def run(get_next, machine, T, initial_rewards): 
    best_lever = np.argmax(machine)
    N = machine.size
    choices = np.zeros(N)
    mean_rewards = initial_rewards(N)
    max_exp_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    for t in range(T):
        max_exp_rewards[t] = operate(machine, best_lever)
        lever = get_next(mean_rewards, choices)
        earned_rewards[t] = operate(machine, lever)
        mean_rewards[lever] = (mean_rewards[lever]*choices[lever] + earned_rewards[t])/(choices[lever]+1)
        choices[lever] += 1
    regret = max_exp_rewards - earned_rewards
    cum_regret = np.cumsum(regret)
    cum_gain = np.cumsum(earned_rewards)    
    
def bernoulli(param):
    return np.random.uniform() < param

def generate_bernoulli_params(N,max_prop=1.):
    params = np.zeros(N)
    params[0] = np.random.uniform()
    params[1:] = np.random.uniform(high=params[0] * max_prop, size=N-1)
    np.random.shuffle(params)
    return params

def zeroed_initial_rewards(N):
    return np.zeros(N)

def random_initial_rewards(N):
    return np.random.uniform(size=N)
        
"""
Bernoulli 
                regret
génération u_i
UCB
"""
