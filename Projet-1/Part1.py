import numpy as np
import matplotlib.pyplot as plt
import argparse

epsilon = 0.1
 
def operate(machine, lever):
    return bernoulli(machine[lever])

def random_algo(rewards, choices):
    return np.random.randint(rewards.size,size=1)[0]

def greedy_algo(rewards, choices):
    return np.argmax(rewards)

def eps_greedy_algo(rewards, choices):
    if bernoulli(epsilon) is True:
        return random_algo(rewards, choices)
    return greedy_algo(rewards, choices)

def ucb_algo(rewards, choices):
    if (np.min(choices) == 0):
        return np.argmin(choices)
    t = np.sum(choices)
    choice_values = rewards + np.sqrt((2 * np.log(t)) / choices)
    return np.argmax(choice_values)
        
def run(get_next, machine, T, initial_rewards): 
    best_lever = np.argmax(machine)
    choices = np.zeros_like(machine)
    mean_rewards = initial_rewards
    max_exp_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    for t in range(T):
        max_exp_rewards[t] = operate(machine, best_lever)
        lever = get_next(mean_rewards, choices)
        earned_rewards[t] = operate(machine, lever)
        mean_rewards[lever] = (mean_rewards[lever]*choices[lever] + earned_rewards[t])/(choices[lever]+1)
        choices[lever] += 1
    regret = max_exp_rewards - earned_rewards
    return np.cumsum(regret), np.cumsum(earned_rewards)
    
def bernoulli(param):
    return np.random.uniform() < param

def generate_bernoulli_params(N, max_prop=1.):
    params = np.zeros(N)
    params[0] = np.random.uniform()
    params[1:] = np.random.uniform(high=params[0] * max_prop, size=N-1)
    np.random.shuffle(params)
    return params

def zeroed_initial_rewards(N):
    return np.zeros(N)

def random_initial_rewards(N):
    return np.random.uniform(size=N)
        
def plot(xlabel, ylabel, title, legend, ys, x=None):
    if x is None:
        x = np.arange(ys.shape[1])
    for y in ys:
        plt.plot(x, y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.legend(legend)
    plt.show()
    
def main():
    learning_algos = [random_algo, greedy_algo, eps_greedy_algo, ucb_algo]
    ap = argparse.ArgumentParser()
    ap.add_argument('levers', type=int, help='the number of levers')
    ap.add_argument('epochs', type=int, help='the number of epochs to simulate')
    ap.add_argument('-r', '--rewards', required=False, help='initial rewards: 0 = zeros (default)\n\t\t1 = random')
    ap.add_argument('-p', '--proportion', required=False, help='maximum proportion of the second highest reward to the highest one')
    args = ap.parse_args()
    N = args.levers
    T = args.epochs
    if (args.rewards is not None) and (int(args.rewards) == 1):
        initial_rewards = random_initial_rewards(N)
    else:
        initial_rewards = zeroed_initial_rewards(N)
    if args.proportion is not None:
        machine = generate_bernoulli_params(N, float(args.proportion))
    else:
        machine = generate_bernoulli_params(N)
    cum_regrets = np.empty([len(learning_algos), T])
    cum_rewards = np.empty([len(learning_algos), T])
    for i in range(len(learning_algos)):
        cum_regrets[i], cum_rewards[i] = run(learning_algos[i], machine, T, initial_rewards)
    plot('time', 'cumulative regret', 'Cumulative regret over time', ['random', 'greedy', 'eps-greedy', 'ucb'], cum_regrets)
    plot('time', 'cumulative reward', 'Cumulative reward over time', ['random', 'greedy', 'eps-greedy', 'ucb'], cum_rewards)

if __name__ == "__main__":
    main()