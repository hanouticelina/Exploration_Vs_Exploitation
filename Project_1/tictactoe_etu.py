import numpy as np
import copy
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches

import utils as ut

## Constante
OFFSET = 0.2

class State:
    """ Etat generique d'un jeu de plateau. Le plateau est represente par une matrice de taille NX,NY,
    le joueur courant par 1 ou -1. Une case a 0 correspond a une case libre.
    * next(self,coup) : fait jouer le joueur courant le coup.
    * get_actions(self) : renvoie les coups possibles
    * win(self) : rend 1 si le joueur 1 a gagne, -1 si le joueur 2 a gagne, 0 sinon
    * stop(self) : rend vrai si le jeu est fini.
    * fonction de hashage : renvoie un couple (matrice applatie des cases, joueur courant).
    """
    NX,NY = None,None
    def __init__(self,grid=None,courant=None):
        self.grid = copy.deepcopy(grid) if grid is not None else np.zeros((self.NX,self.NY),dtype="int")
        self.courant = courant or 1
    def next(self,coup):
        pass
    def get_actions(self):
        pass
    def win(self):
        pass
    def stop(self):
        pass
    @classmethod
    def fromHash(cls,hash):
        return cls(np.array([int(i) for i in list(hash[0])],dtype="int").reshape((cls.NX,cls.NY)),hash[1])
    def hash(self):
        return ("".join(str(x+1) for x in self.grid.flat),self.courant)

class Jeu:
    """ Jeu generique, qui prend un etat initial et deux joueurs.
        run(self,draw,pause): permet de joueur une partie, avec ou sans affichage, avec une pause entre chaque coup.
                Rend le joueur qui a gagne et log de la partie a la fin.
        replay(self,log): permet de rejouer un log
    """
    def __init__(self,init_state = None,j1=None,j2=None):
        self.joueurs = {1:j1,-1:j2}
        self.state = copy.deepcopy(init_state)
        self.log = None
    def run(self,draw=False,pause=0.5):
        log = []
        if draw:
            self.init_graph()
        while not self.state.stop():
            coup = self.joueurs[self.state.courant].get_action(self.state)
            log.append((self.state,coup))
            self.state = self.state.next(coup)
            if draw:
                self.draw(self.state.courant*-1,coup)
                plt.pause(pause)
        return self.state.win(),log
    def init_graph(self):
        self._dx,self._dy  = 1./self.state.NX,1./self.state.NY
        self.fig, self.ax = plt.subplots()
        for i in range(self.state.grid.shape[0]):
            for j in range(self.state.grid.shape[1]):
                self.ax.add_patch(patches.Rectangle((i*self._dx,j*self._dy),self._dx,self._dy,\
                        linewidth=1,fill=False,color="black"))
        plt.show(block=False)
    def draw(self,joueur,coup):
        color = "red" if joueur>0 else "blue"
        self.ax.add_patch(patches.Rectangle(((coup[0]+OFFSET)*self._dx,(coup[1]+OFFSET)*self._dy),\
                        self._dx*(1-2*OFFSET),self._dy*(1-2*OFFSET),linewidth=1,fill=True,color=color))
        plt.draw()
    def replay(self,log,pause=0.5):
        self.init_graph()
        for state,coup in log:
            self.draw(state.courant,coup)
            plt.pause(pause)

class MorpionState(State):
    """ Implementation d'un etat du jeu du Morpion. Grille de 3X3.
    """
    NX,NY = 3,3
    def __init__(self,grid=None,courant=None):
        super(MorpionState,self).__init__(grid,courant)
    def next(self,coup):
        state =  MorpionState(self.grid,self.courant)
        state.grid[coup]=self.courant
        state.courant *=-1
        return state
    def get_actions(self):
        return list(zip(*np.where(self.grid==0)))
    def win(self):
        for i in [-1,1]:
            if ((i*self.grid.sum(0))).max()==3 or ((i*self.grid.sum(1))).max()==3 or ((i*self.grid)).trace().max()==3 or ((i*np.fliplr(self.grid))).trace().max()==3: return i
        return 0
    def stop(self):
        return self.win()!=0 or (self.grid==0).sum()==0
    def __repr__(self):
        return str(self.hash())

class Puissance4State(State):
    """ Implementation d'un etat du jeu Puissance 4. Grille de 6x7.
    """
    NX,NY = 6,7
    WS = 4
    def __init__(self,grid=None,courant=None):
        super(Puissance4State,self).__init__(grid,courant)
    def next(self,coup):
        state =  Puissance4State(self.grid,self.courant)
        state.grid[coup]=self.courant
        state.courant *=-1
        return state
    def get_actions(self):
        cells = np.where(self.grid == 0)
        xs = np.unique(cells[0])
        vfunc = np.vectorize(lambda x: np.min(np.where(cells[0] == x)))
        ys = vfunc(xs)
        return list(zip(xs,cells[1][ys]))
    def win(self):#TODO
        for i in [-1,1]:
            if ((i*self.grid.sum(0))).max()==4 or ((i*self.grid.sum(1))).max()==4 or ((i*self.grid)).trace().max()==3 or ((i*np.fliplr(self.grid))).trace().max()==3: return i
        return 0
    def stop(self):
        return self.win()!=0 or (self.grid==0).sum()==0
    def __repr__(self):
        return str(self.hash())

class Agent:
    """ Classe d'agent generique. Necessite une methode get_action qui renvoie l'action correspondant a l'etat du jeu state"""
    def __init__(self):
        pass
    def get_action(self,state):
        pass


class RandomPlayer(Agent):
    """
    Class representing a random player.
    This agent draws its next move from a uniform distribution of its possible
    next moves.
    """
    def __init__(self):
        super(Agent,self).__init__()

    def get_action(self, state):
        moves = state.get_actions()
        return moves[random.randrange(0,len(moves))]

class MonteCarloPlayer(Agent):
    """
    Class representing a Monte Carlo player.
    This agent implements a Monte Carlo method for its next move choice,
    i.e. it simulates a large number of possible playouts resulting from the
    choice of a random move at its current state and makes a greedy decision
    regarding its next move
    """
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N # the number of playouts

    def get_action(self, state):
        moves = state.get_actions()
        rewards = np.zeros(len(moves))
        for i in range(self.N):
            mv_index = random.randrange(0,len(moves))
            next_state = state.next(moves[mv_index])
            win,log = Jeu(next_state, RandomPlayer(), RandomPlayer()).run()
            outcome = state.courant * win
            
            # the agent looks to avoid defeat at all cost
            if outcome < 0:
                outcome *= 10
            
            rewards[mv_index] += outcome
        return moves[np.argmax(rewards)]

class UCTPlayer(Agent):
    """
    Class corresponding to an Upper Confidence Tree player.
    This agent implements a Monte Carlo tree search for its next move choice,
    i.e. it simulates a large number of possible outcomes starting from its
    current state and makes use of the UCB formula to choose the best next move
    """
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N # the number of simulations

    def get_action(self, state):
        uct = self.get_uct(state)
        evals = [uct.evaluate_for_dm(mv) for mv in uct.moves]
        mv_index = evals.index(max(evals))
        return uct.moves[mv_index]

    def get_uct(self, state):
        """
        Creates an upper confidence tree by simulating N games starting from
        the given state in order to select the move leading to the best results
        
        -------------------
        args:
            state (State): the state for which an optimal move is sought
        --------------------
        return:
            root (UCTree): the upper confidence tree whose root is the given
            state
        --------------------
        """
        root = ut.UCTree(state)
        for _ in range(self.N):
            moves = []
            leaf = root.select(moves, state.courant)
            win = leaf.simulate(moves)
            root.back_propagate(leaf, moves, win)
        return root

#J1 = MonteCarloPlayer(50)
#J1 = UCTPlayer(100)
#J2 = MonteCarloPlayer(200)
#J3 = UCTPlayer(100)
#Game = Jeu(Puissance4State(),J1,J2)
#c,log = Game.run(draw=True)
#print(log)

xs = np.arange(100, 2001,10)
nb_iter = 50
res = []
for N in xs:
    winner = 0
    for _ in range(nb_iter):
        #J1 = MonteCarloPlayer(N)
        J1 = UCTPlayer(N)
        #J2 = MonteCarloPlayer(N)
        #J2 = UCTPlayer(N)
        J2 = RandomPlayer()
        winner += Jeu(MorpionState(),J1,J2).run()[0]
    res.append(winner/nb_iter)
ys = np.array(res)
#ys = np.empty_like(res)
#for i in range(len(res)):
#    sub = res[:i+1]
#    ys[i] = sum(sub) / len(sub)
print(xs)
print(ys)
ut.plot('N', 'gain cumule', ys, xs)
#----------------------------------------------------TESTS -----------------------------------------------------
def histogram(N,nbIter):
    plt.rcdefaults()
    fig, ax = plt.subplots()
    players = np.array(['Random Player','MonteCarlo Player','Draws'])
    performance = np.arange(len(players))
    for i in range(nbIter):
        J1 = RandomPlayer()
        J2 = MonteCarloPlayer(N)
        winner = Jeu(MorpionState(),J1,J2).run(draw = False)[0]
        if(winner == 1):
            performance[0]+=1
        elif(winner == -1):
            performance[1]+=1
        else:
            performance[2]+=1
    xs = [i + 0.2   for i, _ in enumerate(players)]
    plt.bar(xs, performance)
    plt.ylabel("Number of won games")
    plt.xticks([i + 0.2 for i, _ in enumerate(players)], players)
    plt.show()

def simulate_MonteCarloVsRand(nb_iter,N):
    mean1 = np.zeros(nb_iter)
    mean2 = np.zeros(nb_iter)
    draw = np.zeros(nb_iter)
    xs = np.arange(nb_iter)
    J1 = MonteCarloPlayer(N)
    J2 = MonteCarloPlayer(N)
    winner1 = 0
    winner2 = 0
    draws = 0
    for i in range(1,nb_iter):
        w = Jeu(MorpionState(),J1,J2).run()[0]
        if(w == 1):
            winner1+=1
        elif(w == -1):
            winner2+=1
        else:
            draws+=1
        mean1[i] = float(winner1)/i
        mean2[i] = float(winner2)/i
        draw[i] = float(draws)/i
    plt.plot(xs,mean1,'g-',label = 'MonteCarlo Player 1')
    plt.plot(xs,mean2,'r-',label = 'MonteCarlo Player 2')
    plt.plot(xs,draw,'b-',label = 'Draws')
    plt.legend(["MonteCarlo Player 1", "MonteCarlo Player 2 ", "Draws"])
    plt.show()

def simulate_RandVsRand(nb_Iter):
    mean1 = np.zeros(nb_Iter)
    mean2 = np.zeros(nb_Iter)
    xs = np.arange(nb_Iter)
    J1 = RandomPlayer()
    J2 = RandomPlayer()
    winner1 = 0
    winner2 = 0
    for i in range(1,nb_Iter):
        w = Jeu(MorpionState(),J1,J2).run()[0]
        setValues(w, winner1, winner2)
        mean1[i] = float(winner1)/i
        mean2[i] = float(winner2)/i
    plt.plot(xs,mean1,'g-',label = 'Random Player')
    plt.plot(xs,mean2,'r-',label = 'Random Player')

    plt.legend(["Random Player 1", "Random Player 2"])
    plt.show()

"""xs = np.arange(10, 50,5)
nb_iter = 50
res = []
for N in xs:
    winner = 0
    for _ in range(nb_iter):
        J1 = MonteCarloPlayer(N)
        J2 = MonteCarloPlayer(N)
        winner += Jeu(MorpionState(),J1,J2).run()[0]
    res.append(winner/nb_iter)
ys = np.array(res)
ys = np.empty_like(res)
for i in range(len(res)):
    sub = res[:i+1]
    ys[i] = sum(sub) / len(sub)
#print(xs)
#print(ys)
plot('N', 'gain cumule', ys, xs)"""
"""def main():
    simulate_MonteCarloVsRand(1000,20)
if __name__ == "__main__":
    main()"""
