import numpy as np
import copy
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import math

## Constante
OFFSET = 0.2


class State:
    """ Etat generique d'un jeu de plateau. Le plateau est represente par une matrice de taille NX,NY,
    le joueur courant par 1 ou -1. Une case a 0 correspond a une case libre.
    * next(self,coup) : fait jouer le joueur courant le coup.
    * get_actions(self) : renvoie les coups possibles
    * win(self) : rend 1 si le joueur 1 a gagne, -1 si le joueur 2 a gagne, 0 sinon
    * stop(self) : rend vrai si le jeu est fini.
    * fonction de hashage : renvoie un couple (matrice applatie des cases, joueur courant).
    """
    NX,NY = None,None
    def __init__(self,grid=None,courant=None):
        self.grid = copy.deepcopy(grid) if grid is not None else np.zeros((self.NX,self.NY),dtype="int")
        self.courant = courant or 1
    def next(self,coup):
        pass
    def get_actions(self):
        pass
    def win(self):
        pass
    def stop(self):
        pass
    @classmethod
    def fromHash(cls,hash):
        return cls(np.array([int(i) for i in list(hash[0])],dtype="int").reshape((cls.NX,cls.NY)),hash[1])
    def hash(self):
        return ("".join(str(x+1) for x in self.grid.flat),self.courant)

class Jeu:
    """ Jeu generique, qui prend un etat initial et deux joueurs.
        run(self,draw,pause): permet de joueur une partie, avec ou sans affichage, avec une pause entre chaque coup.
                Rend le joueur qui a gagne et log de la partie a la fin.
        replay(self,log): permet de rejouer un log
    """
    def __init__(self,init_state = None,j1=None,j2=None):
        self.joueurs = {1:j1,-1:j2}
        self.state = copy.deepcopy(init_state)
        self.log = None
    def run(self,draw=False,pause=0.5):
        log = []
        if draw:
            self.init_graph()
        while not self.state.stop():
            coup = self.joueurs[self.state.courant].get_action(self.state)
            log.append((self.state,coup))
            self.state = self.state.next(coup)
            if draw:
                self.draw(self.state.courant*-1,coup)
                plt.pause(pause)
        return self.state.win(),log
    def init_graph(self):
        self._dx,self._dy  = 1./self.state.NX,1./self.state.NY
        self.fig, self.ax = plt.subplots()
        for i in range(self.state.grid.shape[0]):
            for j in range(self.state.grid.shape[1]):
                self.ax.add_patch(patches.Rectangle((i*self._dx,j*self._dy),self._dx,self._dy,\
                        linewidth=1,fill=False,color="black"))
        plt.show(block=False)
    def draw(self,joueur,coup):
        color = "red" if joueur>0 else "blue"
        self.ax.add_patch(patches.Rectangle(((coup[0]+OFFSET)*self._dx,(coup[1]+OFFSET)*self._dy),\
                        self._dx*(1-2*OFFSET),self._dy*(1-2*OFFSET),linewidth=1,fill=True,color=color))
        plt.draw()
    def replay(self,log,pause=0.5):
        self.init_graph()
        for state,coup in log:
            self.draw(state.courant,coup)
            plt.pause(pause)

class MorpionState(State):
    """ Implementation d'un etat du jeu du Morpion. Grille de 3X3.
    """
    NX,NY = 3,3
    def __init__(self,grid=None,courant=None):
        super(MorpionState,self).__init__(grid,courant)
    def next(self,coup):
        state =  MorpionState(self.grid,self.courant)
        state.grid[coup]=self.courant
        state.courant *=-1
        return state
    def get_actions(self):
        return list(zip(*np.where(self.grid==0)))
    def win(self):
        for i in [-1,1]:
            if ((i*self.grid.sum(0))).max()==3 or ((i*self.grid.sum(1))).max()==3 or ((i*self.grid)).trace().max()==3 or ((i*np.fliplr(self.grid))).trace().max()==3: return i
        return 0
    def stop(self):
#        if self.win()!=0 or (self.grid==0).sum()==0:
#            print(self.win(), self.grid[self.grid==0], (self.grid==0).sum())
        return self.win()!=0 or (self.grid==0).sum()==0
    def __repr__(self):
        return str(self.hash())

class Agent:
    """ Classe d'agent generique. Necessite une methode get_action qui renvoie l'action correspondant a l'etat du jeu state"""
    def __init__(self):
        pass
    def get_action(self,state):
        pass


class RandomPlayer(Agent):
    def __init__(self):
        super(Agent,self).__init__()

    def get_action(self, state):
        moves = state.get_actions()
        return moves[random.randrange(0,len(moves))]

class MonteCarloPlayer(Agent):
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N

    def get_action(self, state):
        moves = state.get_actions()
        rewards = np.zeros(len(moves))
        for i in range(self.N):
            mv_index = random.randrange(0,len(moves))
            next_state = state.next(moves[mv_index])
            win,log = Jeu(next_state, RandomPlayer(), RandomPlayer()).run()
            outcome = state.courant * win
            if outcome < 0:
                outcome *= 10
            #rewards[action_index] += state.courant * winner
            rewards[mv_index] += outcome
        return moves[np.argmax(rewards)]

def divide(a, b, dm=False):
    """
    Calculates the division of a by b
    If b = 0, the result depends on the value of dm,
    i.e. if the calculation is intended for decision-making,
    then it will yield 0; infinity, otherwise.
    """
    try:
        return a / b
    except:
        if dm is True:
            return 0.
        return float('Inf')

def heuristic(state, move):
    evaltn = 0
    conf_mc = 0
    conf_amaf = 0
    return evaltn, conf_mc, conf_amaf

# Class representing a couple state-move.
# It contains two counters: one for the number of
# victories and the other one for the total number
# of times this action was selected at this state
class State_Move:
    def __init__(self, state, move, evaltn, confid):
        self._state = state
        self._move = move
        self.count = confid
        self.wins = evaltn * confid

    def __repr__(self):
        return f'{self.wins} / {self.count}'

    def add_match(self, winner):
        self.count += 1
        if winner == 0:
            self.wins += 0.5
        elif self._state.courant == winner:
            self.wins += 1

class UCTree:

    b = 2.
    c = math.sqrt(2.)

    def __init__(self, state, my_id=0):
        self.id = my_id
        self.state = state
        self.moves = state.get_actions()
        self.children = {}
        self.mc = {}
        self.amaf = {}
        for mv in self.moves:
            evaltn, confid_mc, confid_amaf = heuristic(state, mv)
            self.children[mv] = None
            self.mc[mv] = State_Move(state, mv, evaltn, confid_mc)
            self.amaf[mv] = State_Move(state, mv, evaltn, confid_amaf)

    def beta_heur(self, move):
        n_mc = self.mc[move].count
        n_amaf = self.amaf[move].count
        if n_mc == 0:
            return 0.
        return n_amaf / (n_mc + n_amaf + 4 * n_mc * n_amaf * UCTree.b)

    def beta_mc(self, move):
        return 0.

    @property
    def t(self):
        return sum([self.mc[mv].count for mv in self.moves])

    def evaluate(self, move):
        Q_mc = divide(self.mc[move].wins, self.mc[move].count)
        Q_amaf = divide(self.amaf[move].wins, self.amaf[move].count)
        beta = self.beta_heur(move)
        expl = math.sqrt(divide(self.t, self.mc[move].count))
        return (1 - beta) * Q_mc + beta * Q_amaf + UCTree.c * expl

    def evaluate_for_dm(self, move):
        return divide(self.mc[move].wins, self.mc[move].count, True)

    def missing_move(self):
        for mv in self.moves:
            if self.children[mv] is None:
                return mv

    def select(self, moves, player):
        current = self
        state = current.state
        while True:
            # if terminal state
            if current.moves == [] or state.stop():
                return current
            mv = current.missing_move()

            # if a move has not yet been made at this state
            if mv is not None:
                moves.append(mv)
                state = state.next(mv)
                current.children[mv] = UCTree(state, current.id+1)
                return current.children[mv]

            # if non-terminal and complete (all moves selected at least once) node
            sign = player * state.courant
            evals = [sign * current.evaluate(mv) for mv in current.moves]
            mv_index = evals.index(max(evals))
            mv = current.moves[mv_index]
            moves.append(mv)
            state = state.next(mv)

            # if this move has not yet been selected
            if current.children[mv] is None:
                current.children[mv] = UCTree(state, current.id+1)
                return current.children[mv]
            current = current.children[mv]

    def simulate(self, moves):
        """
        Plays out a game, whose intial state is given
        by the state of the root of this tree, between
        two random players

        args:
            moves (list[int]): the list of all the selected moves since
            the very first state of this tree, i.e. that of the root

        return:
            win (int) : the winner of this playout
        """
        win, log = Jeu(self.state, RandomPlayer(), RandomPlayer()).run()
        for _,mv in log:
            moves.append(mv)
        return win

    def back_propagate(self, leaf, moves, win):
        node = self
        t = 0
        while node != leaf:
            move = moves[t]
            node.mc[move].add_match(win)
            exectd = set()
            for mv in moves[t:]:
                if mv not in exectd:
                    exectd.add(mv)
                    node.amaf[mv].add_match(win)
            t += 1
            node = node.children[move]

    def plot_espaces(self):
        for _ in range(self.id):
            print('    ',end='')

    def plot(self):
        self.plot_espaces()
        print(f'({self.id})', self.state)
        for mv in self.moves:
            if self.children[mv] is not None:
                print() ; self.children[mv].plot_espaces()
                print(f'{self.mc[mv]}')
                self.children[mv].plot()


class UCTPlayer(Agent):
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N

    def get_action(self, state):
        uct = self.get_uct(state)
#        print(state)
#        uct.plot()
#        print('========')
        evals = [uct.evaluate_for_dm(mv) for mv in uct.moves]
#        print(state.get_actions())
#        print(evals)
        mv_index = evals.index(max(evals))
#        print('====\n',uct.moves[mv_index],'\n')
        return uct.moves[mv_index]

    def get_uct(self, state):
        root = UCTree(state)
        for _ in range(self.N):
            moves = []
            leaf = root.select(moves, state.courant)
            win = leaf.simulate(moves)
            root.back_propagate(leaf, moves, win)
        return root



def plot(xlabel, ylabel, ys, xs):
#    if x is None:
#        x = np.arange(ys.shape[1])
    plt.plot(xs, ys)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    #plt.legend(legend)
    plt.show()

#J1 = MonteCarloPlayer(50)
J1 = RandomPlayer()
#J2 = MonteCarloPlayer(200)
J3 = UCTPlayer(10)
Game = Jeu(MorpionState(),J1,J3)
c,log = Game.run(draw=True)
print(log)

#xs = np.arange(10, 201,10)
#nb_iter = 20
#res = []
#for N in xs:
#    winner = 0
#    for _ in range(nb_iter):
#        #J1 = MonteCarloPlayer(N)
#        J1 = UCTPlayer(N)
#        #J2 = MonteCarloPlayer(N)
#        #J2 = UCTPlayer(N)
#        J2 = RandomPlayer()
#        winner += Jeu(MorpionState(),J1,J2).run()[0]
#    res.append(winner/nb_iter)
#ys = np.array(res)
##ys = np.empty_like(res)
##for i in range(len(res)):
##    sub = res[:i+1]
##    ys[i] = sum(sub) / len(sub)
#print(xs)
#print(ys)
#plot('N', 'gain cumule', ys, xs)
#----------------------------------------------------TESTS -----------------------------------------------------
def histogram(N,nbIter):
    plt.rcdefaults()
    fig, ax = plt.subplots()
    players=np.array(['Random Player','MonteCarlo Player','Matchs nuls'])
    performance = np.arange(len(players))
    for i in range(nbIter):
        J1 = RandomPlayer()
        J2 = MonteCarloPlayer(N)
        winner = Jeu(MorpionState(),J1,J2).run(draw = False)[0]
        if(winner == 1):
            performance[0]+=1
        elif(winner == -1):
            performance[1]+=1
        else:
            performance[2]+=1
    xs = [i + 0.2   for i, _ in enumerate(players)]
    plt.bar(xs, performance)
    plt.ylabel("nombre de parties gagnÃ©es")
    plt.xticks([i + 0.2 for i, _ in enumerate(players)], players)
    plt.show()

def simulate_MonteCarloVsRand(nb_iter,N):
    mean1 = np.zeros(nb_iter)
    mean2 = np.zeros(nb_iter)
    draw = np.zeros(nb_iter)
    xs = np.arange(nb_iter)
    J1 = MonteCarloPlayer(N)
    J2 = MonteCarloPlayer(N)
    winner1 = 0
    winner2 = 0
    draws = 0
    for i in range(1,nb_iter):
        w = Jeu(MorpionState(),J1,J2).run()[0]
        if(w == 1):
            winner1+=1
        elif(w == -1):
            winner2+=1
        else:
            draws+=1
        mean1[i] = float(winner1)/i
        mean2[i] = float(winner2)/i
        draw[i] = float(draws)/i
    plt.plot(xs,mean1,'g-',label = 'MonteCarlo Player 1')
    plt.plot(xs,mean2,'r-',label = 'MonteCarlo Player 2')
    plt.plot(xs,draw,'b-',label = 'Draws')
    plt.legend(["MonteCarlo Player 1", "MonteCarlo Player 2 ", "Draws"])
    plt.show()

def simulate_RandVsRand(nb_Iter):
    mean1 = np.zeros(nb_Iter)
    mean2 = np.zeros(nb_Iter)
    xs = np.arange(nb_Iter)
    J1 = RandomPlayer()
    J2 = RandomPlayer()
    winner1 = 0
    winner2 = 0
    for i in range(1,nb_Iter):
        w = Jeu(MorpionState(),J1,J2).run()[0]
        setValues(w, winner1, winner2)
        mean1[i] = float(winner1)/i
        mean2[i] = float(winner2)/i
    plt.plot(xs,mean1,'g-',label = 'Random Player')
    plt.plot(xs,mean2,'r-',label = 'Random Player')

    plt.legend(["Random Player 1", "Random Player 2"])
    plt.show()

"""xs = np.arange(10, 50,5)
nb_iter = 50
res = []
for N in xs:
    winner = 0
    for _ in range(nb_iter):
        J1 = MonteCarloPlayer(N)
        J2 = MonteCarloPlayer(N)
        winner += Jeu(MorpionState(),J1,J2).run()[0]
    res.append(winner/nb_iter)
ys = np.array(res)
ys = np.empty_like(res)
for i in range(len(res)):
    sub = res[:i+1]
    ys[i] = sum(sub) / len(sub)
#print(xs)
#print(ys)
plot('N', 'gain cumule', ys, xs)"""
"""def main():
    simulate_MonteCarloVsRand(1000,20)
if __name__ == "__main__":
    main()"""
