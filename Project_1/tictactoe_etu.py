import numpy as np
import copy
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches

## Constante
OFFSET = 0.2


class State:
    """ Etat generique d'un jeu de plateau. Le plateau est represente par une matrice de taille NX,NY,
    le joueur courant par 1 ou -1. Une case a 0 correspond a une case libre.
    * next(self,coup) : fait jouer le joueur courant le coup.
    * get_actions(self) : renvoie les coups possibles
    * win(self) : rend 1 si le joueur 1 a gagne, -1 si le joueur 2 a gagne, 0 sinon
    * stop(self) : rend vrai si le jeu est fini.
    * fonction de hashage : renvoie un couple (matrice applatie des cases, joueur courant).
    """
    NX,NY = None,None
    def __init__(self,grid=None,courant=None):
        self.grid = copy.deepcopy(grid) if grid is not None else np.zeros((self.NX,self.NY),dtype="int")
        self.courant = courant or 1
    def next(self,coup):
        pass
    def get_actions(self):
        pass
    def win(self):
        pass
    def stop(self):
        pass
    @classmethod
    def fromHash(cls,hash):
        return cls(np.array([int(i) for i in list(hash[0])],dtype="int").reshape((cls.NX,cls.NY)),hash[1])
    def hash(self):
        return ("".join(str(x+1) for x in self.grid.flat),self.courant)
            
class Jeu:
    """ Jeu generique, qui prend un etat initial et deux joueurs.
        run(self,draw,pause): permet de joueur une partie, avec ou sans affichage, avec une pause entre chaque coup. 
                Rend le joueur qui a gagne et log de la partie a la fin.
        replay(self,log): permet de rejouer un log
    """
    def __init__(self,init_state = None,j1=None,j2=None):
        self.joueurs = {1:j1,-1:j2}
        self.state = copy.deepcopy(init_state)
        self.log = None
    def run(self,draw=False,pause=0.5):
        log = []
        if draw:
            self.init_graph()
        while not self.state.stop():
            coup = self.joueurs[self.state.courant].get_action(self.state)
            log.append((self.state,coup))
            self.state = self.state.next(coup)
            if draw:
                self.draw(self.state.courant*-1,coup)
                plt.pause(pause)
        return self.state.win(),log
    def init_graph(self):
        self._dx,self._dy  = 1./self.state.NX,1./self.state.NY
        self.fig, self.ax = plt.subplots()
        for i in range(self.state.grid.shape[0]):
            for j in range(self.state.grid.shape[1]):
                self.ax.add_patch(patches.Rectangle((i*self._dx,j*self._dy),self._dx,self._dy,\
                        linewidth=1,fill=False,color="black"))
        plt.show(block=False)
    def draw(self,joueur,coup):
        color = "red" if joueur>0 else "blue"
        self.ax.add_patch(patches.Rectangle(((coup[0]+OFFSET)*self._dx,(coup[1]+OFFSET)*self._dy),\
                        self._dx*(1-2*OFFSET),self._dy*(1-2*OFFSET),linewidth=1,fill=True,color=color))
        plt.draw()
    def replay(self,log,pause=0.5):
        self.init_graph()
        for state,coup in log:
            self.draw(state.courant,coup)
            plt.pause(pause)

class MorpionState(State):
    """ Implementation d'un etat du jeu du Morpion. Grille de 3X3. 
    """
    NX,NY = 3,3
    def __init__(self,grid=None,courant=None):
        super(MorpionState,self).__init__(grid,courant)
    def next(self,coup):
        state =  MorpionState(self.grid,self.courant)
        state.grid[coup]=self.courant
        state.courant *=-1
        return state
    def get_actions(self):
        return list(zip(*np.where(self.grid==0)))
    def win(self):
        for i in [-1,1]:
            if ((i*self.grid.sum(0))).max()==3 or ((i*self.grid.sum(1))).max()==3 or ((i*self.grid)).trace().max()==3 or ((i*np.fliplr(self.grid))).trace().max()==3: return i
        return 0
    def stop(self):
#        if self.win()!=0 or (self.grid==0).sum()==0:
#            print(self.win(), self.grid[self.grid==0], (self.grid==0).sum())
        return self.win()!=0 or (self.grid==0).sum()==0
    def __repr__(self):
        return str(self.hash())

class Agent:
    """ Classe d'agent generique. Necessite une methode get_action qui renvoie l'action correspondant a l'etat du jeu state"""
    def __init__(self):
        pass
    def get_action(self,state):
        pass


class RandomPlayer(Agent):
    def __init__(self):
        super(Agent,self).__init__()

    def get_action(self, state):
        moves = state.get_actions()
        return moves[random.randrange(0,len(moves))]

class MonteCarloPlayer(Agent):
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N

    def get_action(self, state):
        moves = state.get_actions()
        rewards = np.zeros(len(moves))
        for i in range(self.N):
            mv_index = random.randrange(0,len(moves))
            next_state = state.next(moves[mv_index])
            win,log = Jeu(next_state, RandomPlayer(), RandomPlayer()).run()
            outcome = state.courant * win
            if outcome < 0:
                outcome *= 10
            #rewards[action_index] += state.courant * winner
            rewards[mv_index] += outcome
        return moves[np.argmax(rewards)]
    
def heuristic(state, move):
    evaltn = 0
    conf_mc = 0
    conf_amaf = 0
    return evaltn, conf_mc, conf_amaf

class State_Move:
    def __init__(self, state, action, eva, conf):
        self.count = conf
        self.wins = eva * conf
        
    def __repr__(self):
        return f'{self.wins} / {self.count}'
    
def divide(a, b):
    try:
        return a / b
    except:
        return float('Inf')
    
class UCTree:
    
    b = 0.
    
    def __init__(self, state, my_id=0):
        self.id = my_id
        self.state = state
        self.moves = state.get_actions()
        self.childs = {}
        self.mc = {}
        self.amaf = {}
        for mv in self.moves:
            evaltn, conf_mc, conf_amaf = heuristic(state, mv)
            self.childs[mv] = None
            self.mc[mv] = State_Move(state, mv, evaltn, conf_mc)
            self.amaf[mv] = State_Move(state, mv, evaltn, conf_amaf)
            
    def beta(self, move):
        n_mc = self.mc[move].count
        n_amaf = self.amaf[move].count
        return divide(n_amaf, n_mc + n_amaf + 4 * n_mc * n_amaf * UCTree.b)
            
    def evaluate(self, move):
        Q_mc = divide(self.mc[move].wins, self.mc[move].count)
        Q_amaf = divide(self.amaf[move].wins, self.amaf[move].count)
        beta = self.beta(move)
        return (1 - beta) * Q_mc + beta * Q_amaf
            
    def select(self, moves, player):
        current = self
        state = current.state
        while True:
            sign = player * state.courant
            evals = [sign * current.evaluate(mv) for mv in current.moves]
            print(current.moves, current.state.stop()) #TODO how to handle terminal state "current"?
            mv_index = evals.index(max(evals))
            mv = current.moves[mv_index]
            moves.append(mv)
            state = state.next(mv)
            if current.childs[mv] is None:
                current.childs[mv] = UCTree(state, current.id+1)
                return current.childs[mv]
            current = current.childs[mv]
            
    def simulate(self, moves):
        win, log = Jeu(self.state, RandomPlayer(), RandomPlayer()).run()
        for _,mv in log:
            moves.append(mv)
        return win
        
    def back_propagate(self, leaf, moves, win):
        def update(st_mv, player):
            if win == 0:
                st_mv.wins += 0.5
            elif win == player:
                st_mv.wins += 1
            
        node = self
        t = 0
        while node != leaf:
            move = moves[t]
            update(node.mc[move], node.state.courant)
            node.mc[move].count += 1
            exectd = set()
            for mv in moves[t:]:
                if mv not in exectd:
                    exectd.add(mv)
                    update(node.amaf[mv], node.state.courant)
                    node.amaf[mv].count += 1
            t += 1
            node = node.childs[move]
            
    def plot(self):
        print(f'{self.id})', self.state)
        for mv in self.moves:
            print(f'{self.mc[mv]}')
            if self.childs[mv] is not None:
                self.childs[mv].plot()

            
class UCTPlayer(Agent):
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N
        
    def get_action(self, state):
        uct = self.get_uct(state)
        #uct.plot()
        #print('========')
        evals = [uct.evaluate(mv) for mv in uct.moves]
        mv_index = evals.index(max(evals))
        return uct.moves[mv_index]
    
    def get_uct(self, state):
        root = UCTree(state)
        for _ in range(self.N):
            moves = []
            leaf = root.select(moves, state.courant)
            win = leaf.simulate(moves)
            root.back_propagate(leaf, moves, win)
        return root
        
        
    
def plot(xlabel, ylabel, ys, xs):
#    if x is None:
#        x = np.arange(ys.shape[1])
    plt.plot(xs, ys)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    #plt.legend(legend)
    plt.show()

#J1 = MonteCarloPlayer(50)
J1 = RandomPlayer()
#J2 = MonteCarloPlayer(200)
J3 = UCTPlayer(5) 
Game = Jeu(MorpionState(),J1,J3)
c = Game.run(draw = True)[0]

#xs = np.arange(10, 201,10)
#nb_iter = 20    
#res = []
#for N in xs:
#    winner = 0
#    for _ in range(nb_iter):
#        J1 = MonteCarloPlayer(N)
#        J2 = MonteCarloPlayer(N)
#        winner += Jeu(MorpionState(),J1,J2).run()[0]
#    res.append(winner/nb_iter)
#ys = np.array(res)
##ys = np.empty_like(res)
##for i in range(len(res)):
##    sub = res[:i+1]
##    ys[i] = sum(sub) / len(sub)
#print(xs)
#print(ys)
#plot('N', 'gain cumule', ys, xs)