import numpy as np
import copy
import random
import matplotlib.pyplot as plt
import matplotlib.patches as patches

import utils as ut

## Constante
OFFSET = 0.2

class State:
    """ Etat generique d'un jeu de plateau. Le plateau est represente par une matrice de taille NX,NY,
    le joueur courant par 1 ou -1. Une case a 0 correspond a une case libre.
    * next(self,coup) : fait jouer le joueur courant le coup.
    * get_actions(self) : renvoie les coups possibles
    * win(self) : rend 1 si le joueur 1 a gagne, -1 si le joueur 2 a gagne, 0 sinon
    * stop(self) : rend vrai si le jeu est fini.
    * fonction de hashage : renvoie un couple (matrice applatie des cases, joueur courant).
    """
    NX,NY = None,None
    def __init__(self,grid=None,courant=None):
        self.grid = copy.deepcopy(grid) if grid is not None else np.zeros((self.NX,self.NY),dtype="int")
        self.courant = courant or 1
    def next(self,coup):
        pass
    def get_actions(self):
        pass
    def win(self):
        pass
    def stop(self):
        pass
    @classmethod
    def fromHash(cls,hash):
        return cls(np.array([int(i) for i in list(hash[0])],dtype="int").reshape((cls.NX,cls.NY)),hash[1])
    def hash(self):
        return ("".join(str(x+1) for x in self.grid.flat),self.courant)

class Jeu:
    """ Jeu generique, qui prend un etat initial et deux joueurs.
        run(self,draw,pause): permet de joueur une partie, avec ou sans affichage, avec une pause entre chaque coup.
                Rend le joueur qui a gagne et log de la partie a la fin.
        replay(self,log): permet de rejouer un log
    """
    def __init__(self,init_state = None,j1=None,j2=None):
        self.joueurs = {1:j1,-1:j2}
        self.state = copy.deepcopy(init_state)
        self.log = None
    def run(self,draw=False,pause=0.5):
        log = []
        if draw:
            self.init_graph()
        while not self.state.stop():
            coup = self.joueurs[self.state.courant].get_action(self.state)
            log.append((self.state,coup))
            self.state = self.state.next(coup)
            if draw:
                self.draw(self.state.courant*-1,coup)
                plt.pause(pause)
        return self.state.win(),log
    def init_graph(self):
        self._dx,self._dy  = 1./self.state.NX,1./self.state.NY
        self.fig, self.ax = plt.subplots()
        for i in range(self.state.grid.shape[0]):
            for j in range(self.state.grid.shape[1]):
                self.ax.add_patch(patches.Rectangle((i*self._dx,j*self._dy),self._dx,self._dy,\
                        linewidth=1,fill=False,color="black"))
        plt.show(block=False)
    def draw(self,joueur,coup):
        color = "red" if joueur>0 else "blue"
        self.ax.add_patch(patches.Rectangle(((coup[0]+OFFSET)*self._dx,(coup[1]+OFFSET)*self._dy),\
                        self._dx*(1-2*OFFSET),self._dy*(1-2*OFFSET),linewidth=1,fill=True,color=color))
        plt.draw()
    def replay(self,log,pause=0.5):
        self.init_graph()
        for state,coup in log:
            self.draw(state.courant,coup)
            plt.pause(pause)

class MorpionState(State):
    """ Implementation d'un etat du jeu du Morpion. Grille de 3X3.
    """
    NX,NY = 3,3
    def __init__(self,grid=None,courant=None):
        super(MorpionState,self).__init__(grid,courant)
    def next(self,coup):
        state =  MorpionState(self.grid,self.courant)
        state.grid[coup]=self.courant
        state.courant *=-1
        return state
    def get_actions(self):
        return list(zip(*np.where(self.grid==0)))
    def win(self):
        for i in [-1,1]:
            if ((i*self.grid.sum(0))).max()==3 or ((i*self.grid.sum(1))).max()==3 or ((i*self.grid)).trace().max()==3 or ((i*np.fliplr(self.grid))).trace().max()==3: return i
        return 0
    def stop(self):
        return self.win()!=0 or (self.grid==0).sum()==0
    def __repr__(self):
        return str(self.hash())

class Puissance4State(State):
    """ Implementation d'un etat du jeu Puissance 4. Grille de 6x7.
    """
    NX,NY = 6,7
    WS = 4
    def __init__(self,grid=None,courant=None):
        super(Puissance4State,self).__init__(grid,courant)
    def next(self,coup):
        state =  Puissance4State(self.grid,self.courant)
        state.grid[coup]=self.courant
        state.courant *=-1
        return state
    def get_actions(self):
        cells = np.where(self.grid == 0)
        xs = np.unique(cells[0])
        vfunc = np.vectorize(lambda x: np.min(np.where(cells[0] == x)))
        ys = vfunc(xs)
        return list(zip(xs,cells[1][ys]))
    def win(self):#TODO
        for i in [-1,1]:
            if ((i*self.grid.sum(0))).max()==4 or ((i*self.grid.sum(1))).max()==4 or ((i*self.grid)).trace().max()==3 or ((i*np.fliplr(self.grid))).trace().max()==3: return i
        return 0
    def stop(self):
        return self.win()!=0 or (self.grid==0).sum()==0
    def __repr__(self):
        return str(self.hash())

class Agent:
    """ Classe d'agent generique. Necessite une methode get_action qui renvoie l'action correspondant a l'etat du jeu state"""
    def __init__(self):
        pass
    def get_action(self,state):
        pass


class RandomPlayer(Agent):
    """
    Class representing a random player.
    This agent draws its next move from a uniform distribution of its possible
    next moves.
    """
    def __init__(self):
        super(Agent,self).__init__()

    def get_action(self, state):
        moves = state.get_actions()
        return moves[random.randrange(0,len(moves))]

class MonteCarloPlayer(Agent):
    """
    Class representing a Monte Carlo player.
    This agent implements a Monte Carlo method for its next move choice,
    i.e. it simulates a large number of possible playouts resulting from the
    choice of a random move at its current state and makes a greedy decision
    regarding its next move
    """
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N # the number of playouts

    def get_action(self, state):
        moves = state.get_actions()
        rewards = np.zeros(len(moves))
        for i in range(self.N):
            mv_index = random.randrange(0,len(moves))
            next_state = state.next(moves[mv_index])
            win,log = Jeu(next_state, RandomPlayer(), RandomPlayer()).run()
            outcome = state.courant * win
            
            # the agent looks to avoid defeat at all cost
            if outcome < 0:
                outcome *= 10
            
            rewards[mv_index] += outcome
        return moves[np.argmax(rewards)]

class UCTPlayer(Agent):
    """
    Class corresponding to an Upper Confidence Tree player.
    This agent implements a Monte Carlo tree search for its next move choice,
    i.e. it simulates a large number of possible outcomes starting from its
    current state and makes use of the UCB formula to choose the best next move
    """
    def __init__(self, N):
        super(Agent,self).__init__()
        self.N = N # the number of simulations

    def get_action(self, state):
        uct = self.get_uct(state)
        evals = [uct.evaluate_for_dm(mv) for mv in uct.moves]
        mv_index = evals.index(max(evals))
        return uct.moves[mv_index]

    def get_uct(self, state):
        """
        Creates an upper confidence tree by simulating N games starting from
        the given state in order to select the move leading to the best results
        
        -------------------
        args:
            state (State): the state for which an optimal move is sought
        --------------------
        return:
            root (UCTree): the upper confidence tree whose root is the given
            state
        --------------------
        """
        root = ut.UCTree(state)
        for _ in range(self.N):
            moves = []
            leaf = root.select(moves, state.courant)
            win = leaf.simulate(moves)
            root.back_propagate(leaf, moves, win)
        return root
