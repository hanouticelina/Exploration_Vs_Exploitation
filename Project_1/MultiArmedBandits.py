import numpy as np
import matplotlib.pyplot as plt
import argparse

epsilon = 0.1

def operate(machine, lever):
    """
    This method gives a reward for a given lever
    ----------------------------------------------------
    Args:
        - machine : an array of success probabilities for each lever
        - lever : chosen lever
    """
    return bernoulli(machine[lever])

def random_algo(rewards, counters):
    """
     Chooses action using a random agent
     ----------------------------------------------------
     Args:
         - rewards : action-value estimates
         - counters(int) : an array of numbers of times each lever was chosen
    """
    return np.random.randint(rewards.size,size=1)[0]

def greedy_algo(rewards, counters):
    """
    Chooses action using a greedy agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    return np.argmax(rewards)

def eps_greedy_algo(rewards, counters):
    """
    Chooses action using an epsilon-greedy agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if bernoulli(epsilon) is True:
        return random_algo(rewards, counters)
    return greedy_algo(rewards, counters)

def ucb_algo(rewards, counters):
    """
    Chooses action using a UCB agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if (np.min(counters) == 0):
        return np.random.choice(np.where(counters == 0)[0], 1)[0]
    t = np.sum(counters)
    return np.argmax( rewards + np.sqrt((2 * np.log(t)) / counters) )

def run(get_next_lever, machine, T, initial_rewards):
    """
    Runs an armed bandit simulation
    ----------------------------------------------------
    Args:
        - get_next_lever:
        - machine: an array of success probabilities for each lever
        - T(int): number of epochs
        - initial_rewards: initial action-value estimates
    ----------------------------------------------------
    Returns:
        - An array of cumulative regrets
        - An array of earned rewards
    """
    best_lever = np.argmax(machine)
    counters = np.zeros_like(machine)
    estimated_rewards = np.copy(initial_rewards)
    expected_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    counts = np.zeros(T)
    for t in range(T):
        expected_rewards[t] = operate(machine, best_lever)
        lever = get_next_lever(estimated_rewards, counters)
        earned_rewards[t] = operate(machine, lever)
        counters[lever] += 1
        estimated_rewards[lever] += (earned_rewards[t] - estimated_rewards[lever]) / counters[lever]
        if(lever == best_lever) :
            counts[t] = 1
    regret = expected_rewards - earned_rewards
    return np.cumsum(regret), np.cumsum(earned_rewards), counts

def bernoulli(param):
    """
    Generates a random binary reward using a Bernoulli distribution
    ---------------------------------------------------
    Args:
        param(float) : Bernoulli parameters
    """
    return np.random.uniform() < param

def generate_bernoulli_params(N, max_proportion=1.):
    """
    Generates an array of Bernoulli parameters using a Uniform distribution
    ----------------------------------------------------
    Args:
        N(int): the size of the array
        max_proportion(float) : the maximum proportion of the second highest reward to the highest one
    """
    params = np.random.uniform(size=N)
    params.sort()
    proportion = params[-1] * max_proportion / params[-2]
    if proportion < 1.:
        params[:-1] *= proportion
    np.random.shuffle(params)
    return params

def zeroed_initial_rewards(N):
    """
    Initializes the initial action-value estimates with zeros
    ----------------------------------------------------
    Args:
        - N(int) : the size of the array
    Returns:
        - an array of action-value estimates

    """
    return np.zeros(N)

def random_initial_rewards(N):
    """
    Initializes the initial action-value estimates with random values
    ----------------------------------------------------
    Args:
        - N(int) : size of the array
    Returns:
        - an array of action-value estimates
    """
    return np.random.uniform(size=N)

def plot(xlabel, ylabel, legend, ys, x=None):
    if x is None:
        x = np.arange(1,1+ys.shape[1])
    for y in ys:
        plt.plot(x, y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend(legend)
    plt.show()

def main():
    learning_algos = [random_algo, greedy_algo, eps_greedy_algo, ucb_algo]
    ap = argparse.ArgumentParser()
    ap.add_argument('levers', type=int, help='the number of levers')
    ap.add_argument('epochs', type=int, help='the number of epochs to simulate')
    ap.add_argument('samples', type=int, help='the number of execution samples to take for each algorithm')
    ap.add_argument('-e','--epsilons', type=float, nargs='+',
                    help='execute the tests of eps-greedy algorithm for the specified epsilon values')
    ap.add_argument('-r', '--rewards', help='the generation mode for initial rewards',
                    choices=['zeros', 'random'], default='zeros')
    ap.add_argument('-m', '--max-proportion', metavar='PROP',
                    help='the maximum proportion of the second highest reward to the highest one')
    args = ap.parse_args()

    N = args.levers
    T = args.epochs
    S = args.samples
    if args.rewards == 'random':
        initial_rewards = random_initial_rewards(N)
    else:
        initial_rewards = zeroed_initial_rewards(N)
    if args.max_proportion is not None:
        machine = generate_bernoulli_params(N, float(args.proportion))
    else:
        machine = generate_bernoulli_params(N)

    if args.epsilons is None:
        cum_regrets = np.zeros([len(learning_algos), T])
        cum_rewards = np.zeros([len(learning_algos), T])
        for i in range(len(learning_algos)):
            for _ in range(S):
                rgs, rws = run(learning_algos[i], machine, T, initial_rewards)
                cum_regrets[i] += rgs
                cum_rewards[i] += rws
            cum_regrets[i] /= S
            cum_rewards[i] /= S
        plot('number of epochs', 'cumulative regret', ['random', 'greedy', r'$\epsilon$-greedy', 'ucb'], cum_regrets)
        plot('number of epochs', 'cumulative reward', ['random', 'greedy', r'$\epsilon$-greedy', 'ucb'], cum_rewards)
    else:
        global epsilon
        legend = [r'$\epsilon$ = ' + f'{e}' for e in args.epsilons]
        cum_regrets_greedy = np.zeros([len(args.epsilons), T])
        cum_rewards_greedy = np.zeros([len(args.epsilons), T])
        best_action_C = np.zeros((len(args.epsilons),S, T))
        for i in range(len(args.epsilons)):
            epsilon = args.epsilons[i]
            for _ in range(S):
                rgs_eps = run(eps_greedy_algo, machine, T, initial_rewards)[0]
                rws_eps = run(eps_greedy_algo, machine, T, initial_rewards)[1]
                cum_regrets_greedy[i] += rgs_eps
                cum_rewards_greedy[i] += rws_eps
            cum_regrets_greedy[i] /= S
            cum_rewards_greedy[i] /= S
            for s in range(S):
                best_action_C[i,s]= run(eps_greedy_algo, machine, T, initial_rewards)[2]
        best_action_C = best_action_C.mean(axis = 1)
        plot('number of epochs', 'optimal action %', legend, best_action_C)
        #plot('number of epochs', 'cumulative regret', legend, cum_regrets_greedy)
        #plot('number of epochs', 'cumulative reward', legend, cum_rewards_greedy)
        optimistic_initial = initial_rewards + 5
        best_action_opt= np.zeros((len(args.epsilons)-1,S, T))
        for s in range(S):
            epsilon = args.epsilons[0]
            best_action_opt[0,s]= run(eps_greedy_algo, machine, T, optimistic_initial)[2]
            epsilon = args.epsilons[2]
            best_action_opt[1,s] = run(eps_greedy_algo, machine, T, initial_rewards)[2]
        best_action_opt = best_action_opt.mean(axis = 1)
        legendopt = [r'$\epsilon$ = 0 with optimistic initial action-value estimates', r'$\epsilon$ = 0.1 with zeroed initial action-value estimates' ]
        plot('number of epochs', 'optimal action %', legendopt, best_action_opt)
if __name__ == "__main__":
    main()
