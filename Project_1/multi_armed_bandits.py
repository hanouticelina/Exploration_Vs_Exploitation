#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt
import argparse

epsilon = 0.1

def operate(machine, lever):
    """
    This method gives a reward for a given lever
    ----------------------------------------------------
    Args:
        - machine : an array of success probabilities for each lever
        - lever : chosen lever
    """
    return bernoulli(machine[lever])

def random_algo(rewards, counters):
    """
     Chooses action using a random agent
     ----------------------------------------------------
     Args:
         - rewards : action-value estimates
         - counters(int) : an array of numbers of times each lever was chosen
    """
    return np.random.randint(rewards.size,size=1)[0]

def greedy_algo(rewards, counters):
    """
    Chooses action using a greedy agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    return np.argmax(rewards)

def eps_greedy_algo(rewards, counters):
    """
    Chooses action using an epsilon-greedy agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if bernoulli(epsilon) is True:
        return random_algo(rewards, counters)
    return greedy_algo(rewards, counters)

def ucb_algo(rewards, counters):
    """
    Chooses action using a UCB agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if (np.min(counters) == 0):
        return np.random.choice(np.where(counters == 0)[0], 1)[0]
    t = np.sum(counters)
    return np.argmax( rewards + np.sqrt((2 * np.log(t)) / counters) )

def run(get_next_lever, machine, T, initial_rewards):
    """
    Runs an armed bandit simulation
    ----------------------------------------------------
    Args:
        - get_next_lever:
        - machine: an array of success probabilities for each lever
        - T(int): number of steps
        - initial_rewards: initial action-value estimates
    ----------------------------------------------------
    Returns:
        - An array of cumulative regrets
        - An array of earned rewards
    """
    best_lever = np.argmax(machine)
    counters = np.zeros_like(machine)
    estimated_rewards = np.copy(initial_rewards)
    expected_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    opt_action_counts = np.zeros(T)
    for t in range(T):
        expected_rewards[t] = operate(machine, best_lever)
        lever = get_next_lever(estimated_rewards, counters)
        earned_rewards[t] = operate(machine, lever)
        counters[lever] += 1
        estimated_rewards[lever] += (earned_rewards[t] - estimated_rewards[lever]) / counters[lever]
        if lever == best_lever:
            opt_action_counts[t] = 1
    regret = expected_rewards - earned_rewards
    return np.cumsum(regret), np.cumsum(earned_rewards), opt_action_counts

def bernoulli(param):
    """
    Generates a random binary reward using a Bernoulli distribution
    ---------------------------------------------------
    Args:
        param(float) : Bernoulli parameters
    """
    return np.random.uniform() < param

def generate_bernoulli_params(N, max_proportion=1.):
    """
    Generates an array of Bernoulli parameters using a Uniform distribution
    ----------------------------------------------------
    Args:
        N(int): the size of the array
        max_proportion(float) : the maximum proportion of the second highest reward to the highest one
    """
    params = np.random.uniform(size=N)
    params.sort()
    proportion = params[-1] * max_proportion / params[-2]
    if proportion < 1.:
        params[:-1] *= proportion
    np.random.shuffle(params)
    return params
def generate_gaussian_params(N,mu=0.5,sigma=0.1):
    return np.random.normal(mu,sigma,N)


def zeroed_initial_rewards(N):
    """
    Initializes the initial action-value estimates with zeros
    ----------------------------------------------------
    Args:
        - N(int) : the size of the array
    Returns:
        - an array of action-value estimates

    """
    return np.zeros(N)

def optimistic_initial_rewards(N, mu_0=5.):
    """
    Initializes the initial action-value estimates optimistically
    with a value > 1.
    ----------------------------------------------------
    Args:
        -  N(int) : the size of the array
        - mu_0(float) : the optimistic initial average reward
    Returns:
        - an array of action-value estimates

    """
    return mu_0 + np.zeros(N)

def random_initial_rewards(N):
    """
    Initializes the initial action-value estimates with random values
    ----------------------------------------------------
    Args:
        - N(int) : size of the array
    Returns:
        - an array of action-value estimates
    """
    return np.random.uniform(size=N)

def plot(xlabel, ylabel, legend, ys, x=None):
    if x is None:
        x = np.arange(1,1+ys.shape[1])
    for y in ys:
        plt.plot(x, y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend(legend)
    plt.show()

def main():
    learning_algos = [random_algo, greedy_algo, eps_greedy_algo, ucb_algo]
    ap = argparse.ArgumentParser()
    ap.add_argument('levers', type=int, help='the number of levers')
    ap.add_argument('steps', type=int, help='the number of steps to simulate')
    ap.add_argument('samples', type=int, help='the number of execution samples to take for each algorithm')
    ap.add_argument('-e','--epsilons', type=float, nargs='+',
                    help='execute the tests of eps-greedy algorithm for the specified epsilon values')
    ap.add_argument('-r', '--rewards', help='the generation mode for initial rewards',
                    choices=['zeros', 'optimistic', 'random'], default='zeros')
    ap.add_argument('-m', '--max-proportion', type=float, metavar='PROP',
                    help='the maximum proportion of the second highest reward to the highest one')
    args = ap.parse_args()

    N = args.levers
    T = args.steps
    S = args.samples
    if args.rewards == 'random':
        initial_rewards = random_initial_rewards(N)
    elif args.rewards == 'optimistic':
        initial_rewards = optimistic_initial_rewards(N)
    else:
        initial_rewards = zeroed_initial_rewards(N)

    if args.epsilons is None:
        cum_regrets = np.zeros([2,len(learning_algos), T])
        cum_rewards = np.zeros_like(cum_regrets)
        opt_action = np.zeros_like(cum_regrets)
        for _ in range(S):
            if args.max_proportion is not None:
                machine_unif = generate_bernoulli_params(N, args.proportion)
            else:
                machine_unif = generate_bernoulli_params(N)
            machine_norm = generate_gaussian_params(N)
            for i in range(len(learning_algos)):
                rgs, rws, opt = run(learning_algos[i], machine_unif, T, initial_rewards)
                cum_regrets[0,i] += rgs
                cum_rewards[0,i] += rws
                opt_action[0,i] += opt
                rgs_n, rws_n, opt_n = run(learning_algos[i], machine_norm, T, initial_rewards)
                cum_regrets[1,i] += rgs_n
                cum_rewards[1,i] += rws_n
                opt_action[1,i] += opt_n
        cum_regrets[0] /= S
        cum_rewards[0] /= S
        opt_action[0] *= 100 / S
        cum_regrets[1] /= S
        cum_rewards[1] /= S
        opt_action[1] *= 100 / S
        legend_norm = ['random with normal distribution', 'greedy with normal distribution', r'$\epsilon$-greedy with normal distribution', 'ucb with normal distribution']
        legend = ['random', 'greedy', r'$\epsilon$-greedy', 'ucb']
        """plot('Steps', 'Cumulative regret', legend, cum_regrets)
        plot('Steps', 'Cumulative reward', legend, cum_rewards)
        plot('Steps', '% Optimal action', legend, opt_action)"""
        xs = np.arange(T)
        plt.plot(xs,cum_regrets[1,3], 'g-', label="UCB with normal distribution")
        plt.plot(xs, cum_regrets[0,3], 'r-', label="UCB with uniform distribution")
        plt.plot(xs,cum_regrets[1,1], 'b-', label="greedy with normal distribution")
        plt.plot(xs, cum_regrets[0,1], 'm-', label="greedy with uniform distribution")
        plt.legend(["UCB with normal distribution", "UCB with uniform distribution","greedy with normal distribution", "greedy with uniform distribution"])
        plt.xlabel('Steps')
        plt.ylabel('Cumulative regret')
        plt.show()
        """mu, sigma = 0.5, 0.1
        count, bins, _ = plt.hist(machine_norm, 20, normed=True)
        plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *np.exp( - (bins - mu)**2 / (2 * sigma**2) ), linewidth=3, color='y')
        plt.show()"""
    else:
        global epsilon
        legend = [r'$\epsilon$ = ' + f'{e}' for e in args.epsilons]
        cum_regrets_greedy = np.zeros([len(args.epsilons), T])
        cum_rewards_greedy = np.zeros_like(cum_regrets_greedy)
        opt_action_greedy = np.zeros_like(cum_regrets_greedy)

        for _ in range(S):
            if args.max_proportion is not None:
                machine = generate_bernoulli_params(N, args.proportion)
            else:
                machine = generate_bernoulli_params(N)

            for i in range(len(args.epsilons)):
                epsilon = args.epsilons[i]
                rgs_eps, rws_eps, opt_eps = run(eps_greedy_algo, machine, T, initial_rewards)
                cum_regrets_greedy[i] += rgs_eps
                cum_rewards_greedy[i] += rws_eps
                opt_action_greedy[i] += opt_eps

        cum_regrets_greedy /= S
        cum_rewards_greedy /= S
        opt_action_greedy *= 100 / S

        plot('Steps', 'Cumulative regret', legend, cum_regrets_greedy)
        plot('Steps', 'Cumulative reward', legend, cum_rewards_greedy)
        plot('Steps', '% Optimal action', legend, opt_action_greedy)

        optimistic_initial = optimistic_initial_rewards(N)
        best_action_opt = np.zeros((len(args.epsilons)-1,S, T))
        for s in range(S):
            epsilon = args.epsilons[0]
            best_action_opt[0,s]= run(eps_greedy_algo, machine, T, optimistic_initial)[2]
            epsilon = args.epsilons[2]
            best_action_opt[1,s] = run(eps_greedy_algo, machine, T, initial_rewards)[2]
        best_action_opt = best_action_opt.mean(axis = 1)
        legendopt = [r'$\epsilon$ = 0 with optimistic initial action-value estimates', r'$\epsilon$ = 0.1 with zeroed initial action-value estimates' ]
        plot('Steps', '% Optimal action', legendopt, best_action_opt)
if __name__ == "__main__":
    main()
