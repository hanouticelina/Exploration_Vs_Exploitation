#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt
import argparse

epsilon = 0.04

def operate(machine, lever):
    """
    This method gives a reward for a given lever
    ----------------------------------------------------
    Args:
        - machine : an array of success probabilities for each lever
        - lever : chosen lever
    """
    return bernoulli(machine[lever])

def random_algo(rewards, counters):
    """
     Chooses action using a random agent
     ----------------------------------------------------
     Args:
         - rewards : action-value estimates
         - counters(int) : an array of numbers of times each lever was chosen
    """
    return np.random.randint(rewards.size, size=1)[0]

def greedy_algo(rewards, counters):
    """
    Chooses action using a greedy agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if np.max(rewards) == 0:
        return np.random.choice(np.where(rewards == 0)[0], size=1)[0]
    return np.argmax(rewards)

def eps_greedy_algo(rewards, counters):
    """
    Chooses action using an epsilon-greedy agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if bernoulli(epsilon) is True:
        return random_algo(rewards, counters)
    return greedy_algo(rewards, counters)

def ucb_algo(rewards, counters):
    """
    Chooses action using a UCB agent
    ----------------------------------------------------
    Args:
        - rewards : action-value estimates
        - counters(int) : an array of number of times each lever was chosen
    """
    if np.min(counters) == 0:
        return np.random.choice(np.where(counters == 0)[0], size=1)[0]
    t = np.sum(counters)
    return np.argmax( rewards + np.sqrt((2 * np.log(t)) / counters) )

def run(get_next_lever, machine, T, initial_rewards):
    """
    Runs an armed bandit simulation
    ----------------------------------------------------
    Args:
        - get_next_lever:
        - machine: an array of success probabilities for each lever
        - T(int): number of steps
        - initial_rewards: initial action-value estimates
    ----------------------------------------------------
    Returns:
        - An array of cumulative regrets
        - An array of earned rewards
    """
    best_lever = np.argmax(machine)
    counters = np.zeros_like(machine)
    estimated_rewards = np.copy(initial_rewards)
    expected_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    opt_action_counts = np.zeros(T)
    for t in range(T):
        expected_rewards[t] = operate(machine, best_lever)
        lever = get_next_lever(estimated_rewards, counters)
        earned_rewards[t] = operate(machine, lever)
        counters[lever] += 1
        estimated_rewards[lever] += (earned_rewards[t] - estimated_rewards[lever]) / counters[lever]
        if lever == best_lever:
            opt_action_counts[t] = 1
    regret = expected_rewards - earned_rewards
    return np.cumsum(regret), np.cumsum(earned_rewards), opt_action_counts

def run_decreasing_epsilon(machine, T, initial_rewards, gamma):#TODO
    """
    Runs an armed bandit simulation
    ----------------------------------------------------
    Args:
        - get_next_lever:
        - machine: an array of success probabilities for each lever
        - T(int): number of steps
        - initial_rewards: initial action-value estimates
    ----------------------------------------------------
    Returns:
        - An array of cumulative regrets
        - An array of earned rewards
    """
    global epsilon
    best_lever = np.argmax(machine)
    counters = np.zeros_like(machine)
    estimated_rewards = np.copy(initial_rewards)
    expected_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    opt_action_counts = np.zeros(T)
    for t in range(T):
        epsilon = 1 / (t+1)**gamma
        expected_rewards[t] = operate(machine, best_lever)
        lever = eps_greedy_algo(estimated_rewards, counters)
        earned_rewards[t] = operate(machine, lever)
        counters[lever] += 1
        estimated_rewards[lever] += (earned_rewards[t] - estimated_rewards[lever]) / counters[lever]
        if lever == best_lever:
            opt_action_counts[t] = 1
    regret = expected_rewards - earned_rewards
    return np.cumsum(regret), np.cumsum(earned_rewards), opt_action_counts

def bernoulli(param):
    """
    Generates a random binary reward using a Bernoulli distribution
    ---------------------------------------------------
    Args:
        param(float) : Bernoulli parameters
    """
    return np.random.uniform() < param

def generate_bernoulli_params(N, max_proportion=1.):
    """
    Generates an array of Bernoulli parameters using a Uniform distribution
    ----------------------------------------------------
    Args:
        N(int): the size of the array
        max_proportion(float) : the maximum proportion of the second highest reward to the highest one
    """
    params = np.random.uniform(size=N)
    params.sort()
    proportion = params[-1] * max_proportion / params[-2]
    if proportion < 1.:
        params[:-1] *= proportion
    np.random.shuffle(params)
    return params

def zeroed_initial_rewards(N):
    """
    Initializes the initial action-value estimates with zeros
    ----------------------------------------------------
    Args:
        - N(int) : the size of the array
    Returns:
        - an array of action-value estimates

    """
    return np.zeros(N)

def optimistic_initial_rewards(N, mu_0=5.):
    """
    Initializes the initial action-value estimates optimistically
    with a value > 1.
    ----------------------------------------------------
    Args:
        -  N(int) : the size of the array
        - mu_0(float) : the optimistic initial average reward
    Returns:
        - an array of action-value estimates

    """
    return mu_0 + np.zeros(N)

def random_initial_rewards(N):
    """
    Initializes the initial action-value estimates with random values
    ----------------------------------------------------
    Args:
        - N(int) : size of the array
    Returns:
        - an array of action-value estimates
    """
    return np.random.uniform(size=N)

def plot(xlabel, ylabel, legend, ys, x=None):
    if x is None:
        x = np.arange(1,1+ys.shape[1])
    for y in ys:
        plt.plot(x, y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend(legend)
    plt.show()

def main():
    learning_algos = [random_algo, greedy_algo, eps_greedy_algo, ucb_algo]
    ap = argparse.ArgumentParser()
    ap.add_argument('levers', type=int, help='the number of levers')
    ap.add_argument('steps', type=int, help='the number of steps to simulate')
    ap.add_argument('samples', type=int, help='the number of execution samples to take for each algorithm')
    ap.add_argument('-e','--epsilons', type=float, nargs='+',
                    help='execute the tests of eps-greedy algorithm for the specified epsilon values')
    ap.add_argument('-r', '--rewards', help='the generation mode for initial rewards',
                    choices=['zeros', 'optimistic', 'random'], default='zeros')
    ap.add_argument('-m', '--max-proportion', type=float, metavar='PROP',
                    help='the maximum proportion of the second highest reward to the highest one')
    args = ap.parse_args()

    N = args.levers
    T = args.steps
    S = args.samples
    if args.rewards == 'random':
        initial_rewards = random_initial_rewards(N)
    elif args.rewards == 'optimistic':
        initial_rewards = optimistic_initial_rewards(N)
    else:
        initial_rewards = zeroed_initial_rewards(N)

    if args.epsilons is None:
        cum_regrets = np.zeros([len(learning_algos), T])
        cum_rewards = np.zeros_like(cum_regrets)
        opt_action = np.zeros_like(cum_regrets)
        
        for _ in range(S):
            if args.max_proportion is not None:
                machine = generate_bernoulli_params(N, args.max_proportion)
            else:
                machine = generate_bernoulli_params(N)
            
            for i in range(len(learning_algos)):
                rgs, rws, opt = run(learning_algos[i], machine, T, initial_rewards)
                cum_regrets[i] += rgs
                cum_rewards[i] += rws
                opt_action[i] += opt
        
        cum_regrets /= S
        cum_rewards /= S
        opt_action *= 100 / S
        
        legend = ['random', 'greedy', r'$\epsilon$-greedy', 'ucb']
        plot('Steps', 'Cumulative regret', legend, cum_regrets)
        plot('Steps', 'Cumulative reward', legend, cum_rewards)
        plot('Steps', '% Optimal action', legend, opt_action)
    else:
        global epsilon
        legend = [r'$\epsilon$ = ' + f'{e}' for e in args.epsilons] + [r'$\epsilon = \dfrac{1}{t^\gamma}$']
        cum_regrets_greedy = np.zeros([len(args.epsilons), T])
        cum_rewards_greedy = np.zeros_like(cum_regrets_greedy)
        opt_action_greedy = np.zeros_like(cum_regrets_greedy)
        
        cum_regrets_greedy_time = np.zeros([1, T])
        cum_rewards_greedy_time = np.zeros_like(cum_regrets_greedy_time)
        opt_action_greedy_time = np.zeros_like(cum_regrets_greedy_time)
        
        for _ in range(S):
            if args.max_proportion is not None:
                machine = generate_bernoulli_params(N, args.max_proportion)
            else:
                machine = generate_bernoulli_params(N)
            
            for i in range(len(args.epsilons)):
                epsilon = args.epsilons[i]
                rgs_eps, rws_eps, opt_eps = run(eps_greedy_algo, machine, T, initial_rewards)
                cum_regrets_greedy[i] += rgs_eps
                cum_rewards_greedy[i] += rws_eps
                opt_action_greedy[i] += opt_eps
                
            rgs_eps, rws_eps, opt_eps = run_decreasing_epsilon(machine, T, initial_rewards, gamma=0.35)
            cum_regrets_greedy_time += rgs_eps
            cum_rewards_greedy_time += rws_eps
            opt_action_greedy_time += opt_eps
                
        cum_regrets_greedy = np.vstack((cum_regrets_greedy, cum_regrets_greedy_time))
        cum_rewards_greedy = np.vstack((cum_rewards_greedy, cum_rewards_greedy_time))
        opt_action_greedy = np.vstack((opt_action_greedy, opt_action_greedy_time))
        
        cum_regrets_greedy /= S
        cum_rewards_greedy /= S
        opt_action_greedy *= 100 / S
            
        plot('Steps', 'Cumulative regret', legend, cum_regrets_greedy)
        plot('Steps', 'Cumulative reward', legend, cum_rewards_greedy)
        plot('Steps', '% Optimal action', legend, opt_action_greedy)
        
def main2():
    learning_algos = [random_algo, greedy_algo, eps_greedy_algo, ucb_algo]
    ap = argparse.ArgumentParser()
    ap.add_argument('levers', type=int, help='the number of levers')
    ap.add_argument('steps', type=int, help='the number of steps to simulate')
    ap.add_argument('samples', type=int, help='the number of execution samples to take for each algorithm')
    args = ap.parse_args()

    step = args.levers
    T = args.steps
    S = args.samples
    
    N_values = list(range(10, 101, step))
    cum_regrets = np.zeros([len(learning_algos), len(N_values)])
    cum_rewards = np.zeros_like(cum_regrets)
    opt_action = np.zeros_like(cum_regrets)
    for iN in range(len(N_values)):
        initial_rewards = random_initial_rewards(N_values[iN])
        for _ in range(S):
            machine = generate_bernoulli_params(N_values[iN])
            
            for i in range(len(learning_algos)):
                rgs, rws, opt = run(learning_algos[i], machine, T, initial_rewards)
                cum_regrets[i][iN] += rgs[-1]
                cum_rewards[i][iN] += rws[-1]
                opt_action[i][iN] += opt[-1] #TODO take the average of the last k (=10) episodes
        
    cum_regrets /= S
    cum_rewards /= S
    opt_action *= 100 / S
        
    legend = ['random', 'greedy', r'$\epsilon$-greedy', 'ucb']
    plot('Levers', 'Cumulative regret', legend, cum_regrets, N_values)
    plot('Levers', 'Cumulative reward', legend, cum_rewards, N_values)
    plot('Levers', '% Optimal action', legend, opt_action, N_values)

if __name__ == "__main__":
    main()