import numpy as np
import matplotlib.pyplot as plt
import argparse


 
def operate(machine, lever):
    return bernoulli(machine[lever])

def random_algo(rewards, choices):
    return np.random.randint(rewards.size,size=1)[0]

def greedy_algo(rewards, choices):
    return np.argmax(rewards)

def eps_greedy_algo(rewards, choices, epsilon):
    if bernoulli(epsilon) is True:
        return random_algo(rewards, choices)
    return greedy_algo(rewards, choices)

def ucb_algo(rewards, choices):
    if (np.min(choices) == 0):
        return np.argmin(choices)
    t = np.sum(choices)
    choice_values = rewards + np.sqrt((2 * np.log(t)) / choices)
    return np.argmax(choice_values)
        
def run(get_next, machine, T, initial_rewards, eps=None): 
    best_lever = np.argmax(machine)
    choices = np.zeros_like(machine)
    mean_rewards = initial_rewards
    max_exp_rewards = np.zeros(T)
    earned_rewards = np.zeros(T)
    for t in range(T):
        max_exp_rewards[t] = operate(machine, best_lever)
        if(eps is None):
            lever = get_next(mean_rewards, choices)
        else : 
            lever = get_next(mean_rewards, choices, eps)
        earned_rewards[t] = operate(machine, lever)
        mean_rewards[lever] = (mean_rewards[lever]*choices[lever] + earned_rewards[t])/(choices[lever]+1)
        choices[lever] += 1
    regret = max_exp_rewards - earned_rewards
    return np.cumsum(regret), np.cumsum(earned_rewards)
    
def bernoulli(param):
    return np.random.uniform() < param

def generate_bernoulli_params(N, max_prop=1.):
    params = np.zeros(N)
    params[0] = np.random.uniform()
    params[1:] = np.random.uniform(high=params[0] * max_prop, size=N-1)
    np.random.shuffle(params)
    return params

def zeroed_initial_rewards(N):
    return np.zeros(N)

def random_initial_rewards(N):
    return np.random.uniform(size=N)
        
def plot(xlabel, ylabel, legend, ys, x=None):
    if x is None:
        x = np.arange(ys.shape[1])
    for y in ys:
        plt.plot(x, y)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend(legend)
    plt.show()
    
def main():
    learning_algos = [random_algo, greedy_algo, eps_greedy_algo, ucb_algo]
    ap = argparse.ArgumentParser()
    ap.add_argument('levers', type=int, help='the number of levers')
    ap.add_argument('epochs', type=int, help='the number of epochs to simulate')
    ap.add_argument('-r', '--rewards', required=False, help='the generation mode for initial rewards: 0 = zeros (default) / 1 = random')
    ap.add_argument('-p', '--proportion', required=False, help='the maximum proportion of the second highest reward to the highest one')
    args = ap.parse_args()
    N = args.levers
    T = args.epochs
    epsilon = 0.1
    if (args.rewards is not None) and (int(args.rewards) == 1):
        initial_rewards = random_initial_rewards(N)
    else:
        initial_rewards = zeroed_initial_rewards(N)
    if args.proportion is not None:
        machine = generate_bernoulli_params(N, float(args.proportion))
    else:
        machine = generate_bernoulli_params(N)
    """cum_regrets = np.empty([len(learning_algos), T])
    cum_rewards = np.empty([len(learning_algos), T])
    for i in range(len(learning_algos)):
        cum_regrets[i], cum_rewards[i] = run(learning_algos[i], machine, T, initial_rewards)
    plot('time', 'cumulative regret', ['random', 'greedy', 'eps-greedy', 'ucb'], cum_regrets)
    plot('time', 'cumulative reward', ['random', 'greedy', 'eps-greedy', 'ucb'], cum_rewards)"""
    epsilons = [0,0.01,0.1]
    cum_regrets_greedy = np.empty([len(epsilons), T])
    cum_rewards_greedy = np.empty([len(epsilons), T])
    for i  in range(len(epsilons)):
        cum_reg_eps = np.empty([200,T])
        cum_rew_eps = np.empty([200,T])
        for j in range(200):
            cum_reg_eps[j],cum_rew_eps[j] = run(eps_greedy_algo, machine, T, initial_rewards, epsilons[i])
        cum_regrets_greedy[i] = np.mean(np.array([ i for i in cum_reg_eps]), axis=0)
        cum_rewards_greedy[i] = np.mean(np.array([ i for i in cum_rew_eps]), axis=0)
    plot('time', 'cumulative regret', ['epsilon=0', 'epsilon=0.01', 'epsilon=0.1'], cum_regrets_greedy)
    plot('time', 'cumulative reward', ['epsilon=0', 'epsilon=0.01', 'epsilon=0.1'], cum_rewards_greedy)
if __name__ == "__main__":
    main()
