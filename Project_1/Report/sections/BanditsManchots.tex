\newpage
\part{Bandits-manchots}

\section{Description du probl\`eme}
On condi\`ere le probl\`eme d'apprentissage suivant : un agent est confront\'e \`a plusieurs reprises \`a un choix parmi N diff\'erentes actions, chacune des N actions procure une r\'ecompense moyenne $\mu^{i}$ inconnue par l'agent et nous d\'esignons l'action sélectionnée sur le pas de temps t par $a_{t}$, la r\'ecompense correspondante par $r_{t}$ et $N_{T}(a)$ le nombre de fois o\‘u l’action $a$ a \'et\'e choisi jusqu'au temps T.  L'objectif est de maximiser la somme des r\'ecompense obtenue au bout des $T$ premi\‘eres parties, pour cel\`a, l'agent doit identifier l'action au rendement le plus elev\'e $i^{*}$= $\argmax_{i \in {1,\ldots,N}}\mu^{i}$. Les valeurs $\mu^{i}$ etant inconnues, il est donc necessaire de faire des estimations qui doivent  \^etre le plus repr\'esentatives possible des valeurs $\mu^{i}$, notons $\hat{\mu}^{i}$ ces estimations. L'estimation associ\'ee \‘a une action est la r\'ecompense moyenne lorsque cette action est s\'electionn\'ee.
\begin{equation*}
\hat{\mu}^{i} = \frac{1}{N_{T}(a)}\sum_{t=1}^{T} r_{t}\mathbbm{1}_{a_{t} = a}
\end{equation*}

\section{Algorithmes}
Si de nombreux algorithmes ont \'et\'e propos\'es pour r\'esoudre ce dilemme, on se contentera ici d'en citer que quelques uns:
\subsection{Algorithme Al\'eatoire}
l'algorithme al\'eatoire se contente de choisir l’action uniform\'ement au hasard, il effectue uniquement de l'exploitation et est utilisée comme baseline pour les tests présentés par la suite. On peut donc d\'ej\`a pr\'esager que cet algorithme sera forc\'ement le moins optimal.

\subsection{Algorithme Greedy}
L'algorithme Greedy est bas\'e sur une politique de selection tr\`es simple qui consiste \`a s\'electionner l'une des actions ayant la valeur estim\'ee la plus \'elev\'ee :
$a_{t}$ = $\argmax_{i \in {1,\ldots,N}}\hat{\mu}^{i}_{t}$.
Autrement dit, L'algorithme Greedy ne fait qu'\'exploiter les connaissances dont on dispose afin de maximiser la recompense \`a un instant $t$.
\subsection{Algorithme $\varepsilon$-Greedy}
Une approche courante pour trouver un compromis exploitation/exploration est l'algorithme $\varepsilon$-Greedy qui consiste à choisir à l’instant $t$, avec une probabilit\'e 1 - $\varepsilon$, l’action qui maximise le rendement moyen sur les estimations faites jusque là et avec une probabilit\‘e $\varepsilon$, une action uniform\‘ement au hasard. L'avantage de cet algorithme repose sur le fait que dans la mesure o\`u le nombre d'étape d'apprentissage augmente, chaque action sera choisie un très grand nombre de fois assurant ainsi que tous les $\hat{\mu}^{i}$ convergeront vers les $\mu^{i}$. De plus, la valeur de $\varepsilon$ a un fort impact sur l'apprentissage, c'est pour cela qu'on en a test\'e diff\'erentes valeurs (constantes) et même sous la forme d'une fonction décroissante du temps. On peut conj\'ecturer la puissance de cet algorithme et sa capacit\'e à explorer toutes les actions possibles et de prendre des d\'ecisions quasi proches de l’optimum.

\subsection{Algorithme UCB}
L'algorithme UCB est une autre strat\'egie qui permet de trouver une balance entre l'exploration et l'exploitation. L'id\'ee de cet algorithme consiste \`a se fier \`a une borne sup\'erieure de confiance, en effet, une incertitude existera toujours sur l'optimalit\'e des estimations faites jusque l\`a. Les deux algorithmes d\'efinis pr\'ecédemment choisissent toujours l'action qui semble meileure \`a un instant $t$ ou se contentent de choisir des actions al\'eatoirement sans distinction. L'algorithme UCB prend en compte l'optimalit\'e des estimations mais également l'incertitude sur celles-ci. Il selectionne l'action : $a_{t}$ = $\argmax_{i \in {1,\ldots,N}}(\hat{\mu}^{i}_{t} + \sqrt{\frac{2\log(t)}{N_{T}(i)}})$.L'incertitude est mesur\'ee par le deuxi\`eme terme, il permet notamment de privili\'egier les actions le plus d\'efavoris\'ees dans le choix de l'agent.
\newpage
\section{Mise en oeuvre et expérimentations}

\begin{figure}[!h]
\centering
  \begin{center}
    \subfloat[Regret cumulé en fonction du nombre d'étape d'apprentissage]{
      \includegraphics[width=0.5\textwidth]{Report/sections/Figures/Figure1.pdf}
      \label{sub:}
                         }
    \subfloat[Recompenses cumulés en fonction du nombre d'étape d'apprentissage]{
      \includegraphics[width=0.5\textwidth]{Report/sections/Figures/Figure2.pdf}
      \label{sub:}
                        }
    \caption{Performance des quatres algorithmes sur 10 leviers et en moyennant les résultats sur 400 éxecutions}
    \label{fig:}
  \end{center}
\end{figure}
La figure 1 montre le gain et le regret cumulé en fonction de $t$ pour les quatre algorithmes décrits dans la section précédente. Nous avons fait le choix de simuler avec une machine à 10 léviers, chacun suit une loi de Bernoulli avec un paramètre choisi uniformément dans [0,1].
TODO : commenter les courbes ci-dessus. Il me semble qu'on est sensé remarquer une amelioration si on change la distribution de gain. En effet, si on a des gains qui suivent une distribution normale plutot qu'une distribution uniforme aléatoire, on remarquera une legere amelioration des performance pour chaque algorithme. Cela est logique, car l'action optimale est mieux "séparé" des autres actions (ici il serait plus judicieux de favoriser l'exploitation plutot que l'exploration).
\begin{figure}[!h]
\centering
  \begin{center}
    \subfloat[Regret cumulé en fonction du nombre d'étape d'apprentissage]{
      \includegraphics[width=0.5\textwidth]{Report/sections/Figures/Figure3.pdf}
      \label{sub:}
                         }
    \subfloat[Recompenses cumulés en fonction du nombre d'étape d'apprentissage]{
      \includegraphics[width=0.5\textwidth]{Report/sections/Figures/Figure4.pdf}
      \label{sub:}
                        }
    \caption{Performance des algorithmes Greedy et $\epsilon$-Greedy}
    \label{fig:}
  \end{center}
\end{figure}
\newpage
La figure 2 illustre une comparaison entre la méthode $Greedy$ et deux autres méthodes $\epsilon$-$Greedy$ ($\epsilon$=0.01 et $\epsilon$=0.1). On remarque que le regret cumulé et le nombre de récompenses augmentent avec le nombre de pas d'apprentissage, on peut observer également qu'au tout début, l'algorithme $Greedy$ s'améliore en apprentissage au même rythme que les deux autres algorithmes puis on voit une diminution en performance. En effet, la méthode gloutonne effectue uniquement de l'exploitation, autrement dit, elle se contente de choisir l'action la plus prometteuse à un instant donné sans explorer d'autres actions qui pourrait l'être davantage, les echantillonages initiaux qu'elle a effectué se sont avérés sous optimaux par la suite ce qui explique son déclin de performance.
La méthode $\epsilon$-$Greedy$ avec $\epsilon$=0.1 a exploré bien plus que la méthode $\epsilon$=0.01 ce qui explique que la plupart du temps elle identifie l'action optimale assez rapidement. La méthode $\epsilon$-$Greedy$ avec $\epsilon$=0.01 s'améliore en apprentissage plus lentement car elle exploite plus mais à un moment donnée, nous sommes sensés observer un gain de performance de tel sorte qu'elle donne de meilleurs résultats que $\epsilon$-$Greedy$ avec $\epsilon$=0.1.

