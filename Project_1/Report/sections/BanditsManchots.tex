\newpage
\part{Bandits-manchots}

\section{Description du probl\`eme}
On condi\`ere le probl\`eme d'apprentissage suivant : un agent est confront\'e \`a plusieurs reprises \`a un choix parmi N diff\'erentes actions, chacune des N actions procure une r\'ecompense moyenne $\mu^{i}$ inconnue par l'agent et nous d\'esignons l'action sélectionnée sur le pas de temps t par $a_{t}$, la r\'ecompense correspondante par $r_{t}$ et $N_{T}(a)$ le nombre de fois o\‘u l’action $a$ a \'et\'e choisi jusqu'au temps T.  L'objectif est de maximiser la somme des r\'ecompense obtenue au bout des $T$ premi\‘eres parties, pour cel\`a, l'agent doit identifier l'action au rendement le plus elev\'e $i^{*}$= $\argmax_{i \in {1,\ldots,N}}\mu^{i}$. Les valeurs $\mu^{i}$ etant inconnues, il est donc necessaire de faire des estimations qui doivent  \^etre le plus repr\'esentatives possible des valeurs $\mu^{i}$, notons $\hat{\mu}^{i}$ ces estimations. L'estimation associ\'ee \‘a une action est la r\'ecompense moyenne lorsque cette action est s\'electionn\'ee.
\begin{equation*}
\hat{\mu}^{i} = \frac{1}{N_{T}(a)}\sum_{t=1}^{T} r_{t}\mathbbm{1}_{a_{t} = a}
\end{equation*}

\section{Algorithmes}
Si de nombreux algorithmes ont \'et\'e propos\'es pour r\'esoudre ce dilemme, on se contentera ici d'en citer que quelques uns:
\subsection{Algorithme Al\'eatoire}
l'algorithme al\'eatoire se contente de choisir l’action uniform\'ement au hasard. On peut donc d\'ej\`a pr\'esager que cet algorithme sera forc\'ement le moins optimal.
\subsection{Algorithme Greedy}
L'algorithme Greedy est bas\'e sur une politique de selection tr\`es simple qui consiste \`a s\'electionner l'une des actions ayant la valeur estim\'ee la plus \'elev\'ee :
$a_{t}$ = $\argmax_{i \in {1,\ldots,N}}\hat{\mu}^{i}_{t}$.
Autrement dit, L'algorithme Greedy ne fait qu'\'exploiter les connaissances dont on dispose afin de maximiser la recompense \`a un instant $t$.
\subsection{Algorithme $\varepsilon$-Greedy}
Une approche courante pour trouver un compromis exploitation/exploration est l'algorithme $\varepsilon$-Greedy qui consiste \‘a choisir \‘a l’instant $t$, avec une probabilit\'e 1 - $\varepsilon$, l’action qui maximise le rendement moyen sur les estimations faites jusque l\‘a et avec une probabilit\‘e $\varepsilon$, une action uniform\‘ement au hasard. L'avantage de cet algorithme repose sur le fait que dans la mesure o\`u le nombre d'étape d'apprentissage augmente, chaque action sera choisie un nombre infini de fois assurant ainsi que tous les $\hat{\mu}^{i}$ convergeront vers les $\mu^{i}$. On peut conj\'ecturer la puissance de cet algorithme et sa capacit\'e \‘a explorer toutes les actions possibles et de prendre des d\'ecisions quasi proches de l’optimum.
\subsection{Algorithme UCB}
L'algorithme UCB est une autre strat\'egie qui permet de trouver une balance entre l'exploration et l'exploitation. L'id\'ee de cet algorithme consiste \`a se fier \`a une borne sup\'erieure de confiance, en effet, une incertitude existera toujours sur l'optimalit\'e des estimations faites jusque l\`a. Les deux algorithmes d\'efinis pr\'ecedemment choisissent toujours l'action qui semble meileure \`a un instant $t$ ou se contentent de choisir des actions al\'eatoirement sans distinction. L'algorithme UCB prend en compte l'optimalit\'e des estimations mais egalement l'incertitude sur celles-ci. Il selectionne l'action : $a_{t}$ = $\argmax_{i \in {1,\ldots,N}}(\hat{\mu}^{i}_{t} + \sqrt{\frac{2\log(t)}{N_{T}(i)}})$.