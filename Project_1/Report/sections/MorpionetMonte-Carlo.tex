\clearpage
\part{Morpion et Monte Carlo}
\section{Description du problème}
Dans cette partie, on considère le célèbre jeu du Tic Tac Toe qui se joue sur une grille 3$\times$3. Les joueurs alternent tour à tour en plaçant un «X» ou un «O» sur une case vide de la grille. Le premier joueur à aligner trois symboles gagne. Nous allons considérer deux types de joueurs, des joueurs $Random$ qui se contenteront de choisir leurs prochains coups à au hasard et des joueurs qui, eux, utiliseront une méthode de $Monte$ $Carlo$.
L'implémentation de ce jeu est basée sur quatre classes : \verb@State@ qui représente l'état du jeu, une classe \verb@Agent@ qui elle, représente le comportement d'un agent. La méthode \verb@get_action@ permet à l'agent de choisir le prochain coup à jouer.

\section{Joueur Aléatoire}

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{Project_1/Report/sections/Figures/rand_vs_rand.png}
\caption{Espérance de gain pour deux joueurs $Random$}
\label{FigGraphes}
\end{figure}

La stratégie aléatoire consiste à choisir aléatoirement une action parmi les actions possibles, autrement dit, un joueur aléatoire choisira une action en exploitant uniquement les informations du jeu.
On considère une partie comme étant une expérience à deux issues : victoire ou défaite du premier joueur.
On note $X$ la variable aléatoire qui dénote la victoire du Joueur 1 ; en cas de victoire, on donne à $X$ la valeur 1, sinon elle prend la valeur 0. 

Il est évident alors que $X$ suit une loi de Bernoulli. Sur la figure ci-dessus, on peut déduire visuellement le paramètre $p$. En effet, en moyenne, le Joueur 1 a une probabilité
$\mathbb{P}$($X$=1) $\simeq$ $0.58$ de gagner. On en déduit que X suit une loi de Bernoulli de paramètre $p$ $\simeq$ $0.58$ et de variance $\mathbb{V}$($X$)= $p$($1-p$) $\simeq$ $0.24$.

\section{Joueur Monte Carlo}
L'idée générale d'une simulation de $Monte$ $Carlo$ consiste à jouer un certain nombre de parties avec des choix aléatoires puis d'utiliser les résultats de ces parties pour calculer une bonne action à jouer. Lorsqu'un joueur gagne une de ces parties aléatoires, il devra privilégier les actions qui l'ont mené à cette victoire, dans l'espoir de choisir un coup gagnant lors des prochaines parties. Une stratégie $Monte$ $Carlo$ échantillonne aléatoirement l'espace de toutes les actions possibles et fait une estimation de l'action qui semble la plus optimale.
\subsection{Implémentation}
Pour implémenter cet algorithme nous avons créer une classe \newline\verb@MonteCarloPlayer@ qui hérite de \verb@Agent@ et qui implémente la méthode \verb@get_action@ dans laquelle on recupère d'abord toutes les actions possibles. Ensuite, pour $N$ itérations, on choisit une action aléatoirement et on recupère l'état du jeu engendré qui sera simulé jusqu'à terminaison par deux joueurs aléatoires. Enfin, on met à jour la récompense associée à cette action suivant le résultat de la simulation. La méthode renvoie l'action avec la plus grande récompense.
\subsection{Mise en oeuvre}

\begin{figure}[!h]
\centering
  \begin{center}
    \subfloat[Espérance de gain lorsqu'il commence les parties][Espérance de gain du Joueur Monte Carlo avec un nombre d'échantillonnage égale à 100 et lorsqu'il entame les parties]{
      \includegraphics[width=0.5\textwidth]{Project_1/Report/sections/Figures/mc_first.png}
      \label{sub:}
                         }
    \subfloat[Espérance de gain lorsqu'il joue en deuxième][Espérance de gain du Joueur Monte Carlo avec un nombre d'échantillonnage égale à 100 et lorsque c'est le joueur adverse qui entame les partie]{
      \includegraphics[width=0.5\textwidth]{Project_1/Report/sections/Figures/mc_last.png}
      \label{sub:s}
                        }
    \caption[Benchmark du joueur $Monte$ $Carlo$]{Comparaison entre un joueur $Monte$ $Carlo$ et trois joueurs: Un joueur $Random$, un joueur $UCT$ un un joueur suivant la même stratégie $Monte$ $Carlo$}
    \label{fig:renonculacees}
  \end{center}
\end{figure}

On peut observer sur la figure 6-$a$ que le joueur basé sur la méthode $Monte$ $Carlo$ surpasse largement le joueur $Random$ avec plus de 95\% de parties gagnées. Cependant, il existe des situations où gagner est impossible pour le joueur $Monte$ $Carlo$, malgré le fait que sa politique de jeu soit plus intelligente que celle du joueur $Random$. Par exemple, la défaite est inévitable si c'est le joueur $Random$ qui entame la partie et qu'il marque la case centrale en premier. Dans ce cas, beaucoup de voies seront bloquées dans la grille et donc, peu importe la stratégie du joueur, dans le meilleur cas, il fera match nul. Par ailleurs, $Monte$ $Carlo$ gagne entre 50\% et 70\% des parties jouées contre un joueur $UCT$ et un autre joueur $Monte$ $Carlo$ avec le même nombre d'échantillonnage.

La figure 6-$b$ illustre très bien l'avantage donné au joueur qui entame la partie étant donnée que l'espérance de gain du joueur $Monte$ $Carlo$ diminue considérablement face aux deux joueurs $Monte$ $Carlo$ et $UCT$ et diminue légèrement face au joueur $Random$. Cela peut s'expliquer, comme précédemment, par le fait qu'il est très difficile pour un joueur de gagner lorsqu'il n'entame pas la partie malgré le fait que sa stratégie de jeu soit plus optimale que celle de son adversaire. Une hiérarchie entre les trois stratégies peut donc être déduite de ces courbes, cependant, ces résultats expérimentaux sont toutefois dépendants d'autres paramètres comme le nombre d'échantillonnages et le facteur d'exploration de l'algorithme $UCT$.